**观前提醒：如果只是想了解个大概可以不用看斜体部分（虽然也很简单）**

# 什么是无监督学习？

用于学习的数据没有任何标记，机器完全不知道是什么

## 先问什么是学习

- 学习的本质是举一反三：以高考为例，高考的题目在上考场前我们未必做过，但在高中三年我们做过很多很多题目，懂解题方法，因此考场上面对陌生问题也可以算出答案
- 还原到深度学习训练模型中来：
  - **【注意：这个比喻是对于抽象的有无监督学习而言的，不能直接套用于某一种学习】**
    - ~~某一种学习 = 一种做题方式？不太好想~~
    - 某一个问题/情景/应用 = 一种特定的题型

  - 深度学习：我们能不能利用一些训练数据（已经做过的题），使机器能够利用它们（解题方法）分析未知数据（高考的题目）
  - 有无监督：这些训练数据有没有打标签（已知在考纲内/不知道在不在考纲里）
    - 有监督：数据都有标签（都做的是在考纲内的题），做高考题正确率就高（指向性强），但是老师累（打标签开销大）
    - 无监督：数据都没有（题目不知道在不在考纲，乱做），做高考题随缘了（没有目的性），但是老师轻松（不用处理数据）
    - 半监督：数据有一小部分有标签（老师先给了一些高考真题），后面做题可以自己判断在不在考纲，做高考题就好很多了

- 因此这个**有无监督**是针对于**训练算法的模型**（教做题家的老师）的，而不是训练出的算法本身（真正做题的学生）

## 无监督学习是干嘛的

我们就拿训练分类算法的模型来举例。

- 什么是分类？就是已知Label（要分成几类）和Feature（有什么特点），**找出描述Label和Feature关系的函数**，通过Label实现分类（也就是说Label是答案）
  - 但是，打标签太麻烦了，这种重复机械的工作就该交给机器做

- 这就是无监督学习的目的，已知Feature但没有Label，通过已有的关系得到未知数据标签 
  - 我们把无监督分类又叫聚类算法
  - 为什么叫聚类呢？因为我们完全不监督，就是一个标签也没有，机器也不知道我们想要什么，他只能觉得哪些长得像就放在一堆
  - 无监督分类（聚类算法）的特征
    - 无法预知结果，可用性可能较低
    - 可以统计地从完全没有标签的数据中发现新的潜在结构（比如可以发现洗钱这种小规模异常行为）

#### 有没有办法有方向又不用打标签呢

那肯定是有的，就是**半监督学习**（无标签的数据远大于有标签的数据）

- 为什么可以呢？
  - 真实数据的分布肯定不是随机的
  - 可以基于有标签部分的结构推测全体数据结构，优化完全无监督学习的结果

### 常见的无监督学习算法应用

先看无监督学习两种最常见的应用

- 降维：提取有效信息，删除无效信息
- 聚类（刚刚讲过了）：所有算法均假设：数据点已经预先分布在了向量空间中
  - *K均值聚类*
    - *先随机定义K个重心（或者其他算法）*
    - *每个数据点分配给最近的重心*
    - *将重心移动到聚类的中心，再重新上述步骤，直到重心位置不再变化*
  - *层次聚类：不知道多少类*
    - *每个数据点分配一个聚类*
    - *将彼此最近的两个聚类合并*
    - *重新计算，重复上述步骤，直到收缩到想要的类别数量*

## 来看几个经典模型

说了这么多，那来看看模型吧。

首先呢，这个无监督学习分为两个大类：生成学习和对比学习

<img src="C:/Users/kirito/AppData/Roaming/Typora/typora-user-images/image-20220413183724926.png" alt="image-20220413183724926" style="zoom: 67%;" />

知己知彼，百战不殆，今天要讲对比学习，那就首先从**生成学习**里挑一个大家最熟悉的来讲

### 生成对抗网络 GAN

无监督学习要一堆没有标签的数据，哪里来呢？这不简单，自己造呗。

GAN就训练了一个算法来生成自己的数据实例。

<img src="https://pic2.zhimg.com/v2-8e747f48105e3507e9d64c2faaab78ad_r.jpg" alt="img" style="zoom:50%;" />

先不慌看这个模型在干嘛，我们先高屋建瓴地看一下生成模型的特点，尤其是和对比模型的不同点。

仔细看我们这个模型最后训练出来的东西干了啥？G从隐空间中生成了一张图，D判断后还是输出了一张图。生成模型和对比模型最大的区别就在这了，我输进去一个啥，他出来还是一个啥，至少类型还是一样的。

至于GAN具体实现什么，我们也不用管了，反正我们现在也用不上他。

再举一个生成模型的例子：**自动编码器**。

我们也不用管具体长什么样，只需要知道他把元数据经过一个ENCODER->LATENT VECTOR->DECODER的过程，把原数据降噪了，降维了就可以。发现了吗，还是进去是啥就出来是啥。

### 引入一点对比学习

回想一下生成学习，是不是输进去**一个**东西，处理以后提取有效信息，还得保证出来的还是这个东西。

没错，可我们如果最终只是想分个类，我们没有必要对他本身动手脚是吧，我们只需要找到一个**合适的方法去比较他们中的每两个**（划重点！下一节要说！），最后把他们相似的摆成一堆就好了呀。

这里就能引出对比学习的一个特点：不需要关注实例上繁琐的细节（**不用对向量本身修改**），只需要在抽象语义级别的特征空间上学会对数据的区分（**构建对向量空间整体的变换**）



# 小小的总结一下

生成模型我们说的简单一点，就是生成一个编码器，这个编码器会接受原数据，然后输出一系列Feature Key，把多余信息剃掉剃掉。

- 比如我输入一张充满噪点的照片，理想情况下输出的应该是一张去除了噪点的照片。

- 其无监督的特点主要体现在模型的训练过程中，不必细究

而对比模型呢，也是一个编码器，它没那么不要求数据本身不变，只要求**不同的样本之间差别越大越好**。

我们把这个情景代入到向量空间模型中，在对比模型中，我们可以认定每个样本已经以某种形式客观存在于空间中，我们（机器）也不用关心它具体是什么样，只需要找到一个合适的锚点（原点）来确定一个向量空间，在这个空间中相似的样本距离足够近，就得到了符合要求的编码器。

~~（多余的话）~~这样就把对向量本身进行特征提取和编码的工作变成了求一个合适的向量空间变换算法的工作。

