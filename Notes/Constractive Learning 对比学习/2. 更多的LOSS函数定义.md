# 前情提要



# 各种LOSS函数的计算方法



---

### Triplet Loss 三元组模型

![image-20220409151948020](C:/Users/kirito/AppData/Roaming/Typora/typora-user-images/image-20220409151948020.png)

Triplet Loss认为，假如所有正样本之间无限的拉近，会导致聚类过拟合，所以，就只要求锚点到负样本之间的距离比到正样本之间差距足够大即可。

三元组有三种状态

- easy triplets: 满足条件
- semi-hard triplets: 正样本离锚点的距离比负样本小，但未满足条件
- hard triplets: 正样本离锚点的距离比负样本还大

---

### NCE Loss（Noise Contrastive Estimation）

Reference：

- [Noise Contrastive Estimation 学习 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/58369131)
- 

与前面方法的不同之处：采用概率的思想，把多分类问题转化为二分类，如之前的问题是计算某个类的归一化概率是多少，二分类的问题是input和label正确匹配的概率是多少

样例：手写数字数据集的TensorFlow判断

NCE强大之处真的不只是能够**解决巨大词表Softmax的运算量的问题（规避Sofmax运算）**，而是在于它能够解决归一化项中积分（而非求和）无法计算的问题，毕竟如果能够用采样替代计算整个积分，这玩意就能用来对生成模型进行建模了（例如GAN）

#### 实现思路

- 将原有的判定从0/1，变成选择某一项的概率
- 为了保证数值可用，需要对概率归一化处理，但计算开销太大
- 在样本空间中**加入随机生成的假样本**可以解决上述问题（Why？我也不知道，论文也没证，我也不关心）

#### Softmax（归一化指数函数）

他把一些输入映射为0-1之间的实数，并且归一化保证和为1，因此多分类的概率之和也刚好为1

**本质：归一化的分类概率分布**



---

### InfoNCE Loss



