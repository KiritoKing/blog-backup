# MG Lattice 模型

该模型来自论文 [Chinese Relation Extraction with Multi-Grained Information and External Linguistic Knowledge](http://nlp.csai.tsinghua.edu.cn/~lzy/publications/acl2019_nre4chinese.pdf)

### 特点

- 同时利用字级、词级和额外的词义信息，来解决句子的多义词划分问题
- 有监督学习：训练数据已经打好了标签

### 工作原理

![在这里插入图片描述](https://picgo-1308055782.cos.ap-chengdu.myqcloud.com/typora-bed/%2020200623001340986.PNG)

#### Input Representation 输入层

主要用于计算模型的输入向量

1. 直接输入（Lexicon）是文本中所有字向量（逐字分），词向量（直接匹配）是间接输入，每个句子视为一个字符序列
   - 词向量来源：潜在词信息（*原始文本中与现有词典D中单词匹配的任何字符子序列*）
   - 将所有潜在词通过 word2vec 转换成一个实值变量
   - 通过外部知识库查询给定词的词义信息集合，使用SAT将每个词义转换成实值向量
   - 最后把**字向量、词向量和词义向量都要输入**
2. 将每个字符ci映射成一个 d^c^ 维度的矢量（how？），表示为 xi^ce^
3. 利用位置嵌入（从当前字符到实体的头部和尾部的相对距离）在表示实体在句子中的位置![在这里插入图片描述](https://picgo-1308055782.cos.ap-chengdu.myqcloud.com/typora-bed/%2020200222231634659.PNG)
4. 把pi^1^转换成位置向量 xi^p1^ 并拼接依次拼接在字向量 xi^ce^ 后形成一个**关系三元组**（？）向量



#### MG Lattice Encoder 编码层

编码层对输入层得到的3类向量（字向量、词向量和词义向量）进行处理，分为 Basic Lattice LSTM Encoder 层（处理字向量和词向量）和 MG Lattice Encoder 层（处理词义向量）



##### Basic Lattice LSTM Encoder

**Lattice LSTM** 同时用Character和Word信息进行提取

![img](https://picgo-1308055782.cos.ap-chengdu.myqcloud.com/typora-bed/%20v2-9d1ee21972454d6b2ad8974e9ccda03d_r.jpg)

图中红色的Cell是xi,j可能构成的词组（潜在词信息，从输入层得到），这些cell会与字向量融合，融合方法被称为Lattice

###### **LSTM结构解析**

LSTM（长短期记忆网络，Long-Short-Term Memory），是一种特殊的RNN（循环神经网络），能够避免长期依赖性，其长时间记住信息是默认能力

RNN与传统CNN最大的不同是会将隐藏层的输出返回重新作为隐藏层的输入，每一层的输入都会影响后面的输出，最后只需要输出最后一次的输出即可代表整个序列，这让RNN在序列处理中有独特的优势。

但RNN存在一个问题，越晚的输入影响越大，越早的输入影响越小，且无法改变这个逻辑，LSTM改变了这个问题，他只保留最重要的信息（由重要性判断影响而不是序列顺序）

LSTM的关键是细胞状态，通过门结构向细胞状态添加或移除信息，得以选择性地让信息通过（由S型神经网络层和逐点乘法运算组成）

- 所有RNN都有神经网络的链式重复模块，普通RNN中这个结构很简单，但LSTM中这个模块有更复杂的结构

  ![LSTM有4个神经网络层](https://picgo-1308055782.cos.ap-chengdu.myqcloud.com/typora-bed/%202019-07-05-lstm.png)

- 每个门由**一个S型网络和一个逐点乘法构成**：S型网络的值介于0~1之间，表示有多大比例的信息通过（重要性），x代表逐点乘法，σ代表s型网络

  <img src="https://picgo-1308055782.cos.ap-chengdu.myqcloud.com/typora-bed/%202019-07-05-men.png" alt="LSTM 可以通过所谓“门”的精细结构向细胞状态添加或移除信息" style="zoom: 50%;" />

- t-1时刻输出Cell State(xt-1)和Hidden State(ht-1)作为t时刻的输入

- 一个LSTM有三个这样的门来保持和控制细胞状态

  - 遗忘门：第一个门，决定Cell State要保留多少信息

  - 输入门：第二个门，对Cell State进行更新

  - 输出门：第三个门，决定更新后的Cell State有多少可以更新

  - 计算过程

    ![image-20220603163526994](https://picgo-1308055782.cos.ap-chengdu.myqcloud.com/typora-bed/%20image-20220603163526994.png)

    - i, o , f 分别对应输入门、输出门和遗忘门，c是对Cell State的更新

###### **回到Lattice LSTM**

基于字的LSTM（蓝色Cell）完全可以沿用普通LSTM的结构

基于词的LSTM则有以下变化：（红色Cell）不输出Hidden State，只传递Cell State

![image-20220603164136782](https://picgo-1308055782.cos.ap-chengdu.myqcloud.com/typora-bed/%20image-20220603164136782.png)

- 取消了输出门
- h, c来自词组首字的Hidden State和Cell State

因此一个字现在有两大部分的Cell State信息（字信息+词信息，其中词信息可能有多义词对应多个Cell State），文章中使用**加权融合**（递归过程中是算完全部需要的再进行下一步计算吗？）进行融合（类似Softmax）



##### MG Lattice LSTM Encoder

这一层中加入词义向量，方法和Lattice LSTM差不多，并求出多个词义的权重α方便加进模型

![image-20220603164602586](https://picgo-1308055782.cos.ap-chengdu.myqcloud.com/typora-bed/%20image-20220603164602586.png)



#### Relation Classifier 关系分类层

将编码层得到的隐藏层作为输入（如何处理没有隐藏层的词组cell？），经过Attention计算来进行关系分类

这里使用的关系分类的方法，借鉴了16年的ACL论文中提出的Att-BiLSTM模型的关系分类方法，只不过是将BiLSTM换成了MGLatticeLSTM。Att-BiLSTM中提出的了基于自注意力机制的关系分类方法，有效提升了关系分类的性能

![image-20220603165038123](https://picgo-1308055782.cos.ap-chengdu.myqcloud.com/typora-bed/%20image-20220603165038123.png)


