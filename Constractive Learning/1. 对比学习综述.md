# 背景知识

参考文献：[对比学习（Contrastive Learning）综述](https://zhuanlan.zhihu.com/p/346686467)

对于深度学习所用数据，根据标记与否可以分为**监督学习**和**无监督学习**两种

监督学习：需要对海量数据进行标记

无监督学习：自主发现数据中潜在的结构，可以自主地从大量数据中学习同类数据的相同特性，并将其编码为高级特征，再根据不同任务进行微调

Q：两种学习方式的优缺点

- 生成式学习：以自编码器方法为代表，由数据生成数据，使之在整体上与训练数据相近
- 对比式学习：着重学习同类实例之间的共同特征，区分非同类实例之间的不同之处
  - 不需要关注实例中繁琐的细节，只需要在抽象语义级别的特征空间上学会对数据的区分
  - 学习目标是一个编码器，对同类数据进行相似的编码，并使不同类的数据的编码结果尽可能不同



# 对比学习 Contrastive Learning

对比思想：一种无监督学习范式，缩小与正样本间距离，扩大与负样本间距离，最终使得正样本与锚点的距离远小于负样本与锚点的距离

<img src="https://pic4.zhimg.com/v2-a17404a49d4f980ac69653464dbcc3fb_r.jpg" alt="img" style="zoom: 50%;" />



## 对比损失（Loss函数）

通过定义Loss函数来表达锚点（某一向量组的欧氏距离）到正/负样本的距离，其代表了编码分类的不准确度，起到自我校正的作用。



### 对比平衡关系（力学模型）

![image-20220409151606174](C:/Users/kirito/AppData/Roaming/Typora/typora-user-images/image-20220409151606174.png)

---

### Triplet Loss 三元组模型

![image-20220409151948020](C:/Users/kirito/AppData/Roaming/Typora/typora-user-images/image-20220409151948020.png)

Triplet Loss认为，假如所有正样本之间无限的拉近，会导致聚类过拟合，所以，就只要求锚点到负样本之间的距离比到正样本之间差距足够大即可。

三元组有三种状态

- easy triplets: 满足条件
- semi-hard triplets: 正样本离锚点的距离比负样本小，但未满足条件
- hard triplets: 正样本离锚点的距离比负样本还大

---

### NCE Loss（Noise Contrastive Estimation）

Reference：

- [Noise Contrastive Estimation 学习 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/58369131)
- 

与前面方法的不同之处：采用概率的思想，把多分类问题转化为二分类，如之前的问题是计算某个类的归一化概率是多少，二分类的问题是input和label正确匹配的概率是多少

样例：手写数字数据集的TensorFlow判断

NCE强大之处真的不只是能够**解决巨大词表Softmax的运算量的问题（规避Sofmax运算）**，而是在于它能够解决归一化项中积分（而非求和）无法计算的问题，毕竟如果能够用采样替代计算整个积分，这玩意就能用来对生成模型进行建模了（例如GAN）

#### 实现思路

- 将原有的判定从0/1，变成选择某一项的概率
- 为了保证数值可用，需要对概率归一化处理，但计算开销太大
- 在样本空间中**加入随机生成的假样本**可以解决上述问题（Why？我也不知道，论文也没证，我也不关心）

#### Softmax（归一化指数函数）

他把一些输入映射为0-1之间的实数，并且归一化保证和为1，因此多分类的概率之和也刚好为1

**本质：归一化的分类概率分布**



---

### InfoNCE Loss





研究重点

定义Loss函数

定义目标函数

构建正实例对和负实例对





# 对比学习模型

#### CPC（Contrastive Predictive Coding）

CPC的**目标**就是要做unsupervised representation learning，并且我们希望这个representation有很好的predictive的能力

对于高维度，少Label的数据集，可以利用无监督学习获取高级信息

这里的Predict是无监督学习的一种方式，最终目的仍然是分类

- 压缩高维数据到更紧凑的**隐空间（Latent Space）**中，便于建模
  - 隐空间本身只是表示压缩后的数据
  - 隐空间的作用是为了找到一个pattern来存储所有相关信息并忽略噪音，达到降维、压缩的效果
  - 忽略多余信息后，特征相似的点在隐空间中的距离更近
  - 从数据空间映射到隐空间，然后再映射回数据空间就可以实现简单的自动编码器
- 利用**自回归模型（AR，Autoregressive Model）**实现预测
  - 统计上一种处理时间序列的方法，用同一变数例如x的之前各期，亦即**x1至xt-1来预测本期xt的表现**，并假设它们为一线性关系。
  - 因为这是从回归分析中的线性回归发展而来，只是不用x预测y，而是用x预测 x（自己）；所以叫做自回归
- 利用NCE计算Loss
- 对于多模态的数据可以学到高级信息



#### Representation Learning 表征学习

**学习一个特征**（**将原式数据转换成能被机器学习和利用的形式**，这个形式就是representation）的技术的集合

它避免了手动提取特征的麻烦，允许计算机学习**使用特征**的同时，也**学习如何提取特征：学习如何学习**

特征学习也分为监督和无监督两种，类似于机器学习



把encoder好不好的问题转换为encoding出来的vector和原数据配对后 真假样本能不能被一个分类器分开